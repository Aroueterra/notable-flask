{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make tf 2.0 compatible with tf1.0 code, we disable the tf2.0 functionalities\n",
    "#tf.disable_eager_execution()\n",
    "def appending(list, element):\n",
    "    return np.concatenate((list, [element]), axis=0) if list is not None else [element]\n",
    "\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "import ctc_utils\n",
    "import numpy as np\n",
    "import silence_tensorflow.auto\n",
    "import tensorflow as tf\n",
    "import simpleaudio as sa\n",
    "from pathlib import Path\n",
    "from midi.player import *\n",
    "import tensorflow.compat.v1 as tf_v1\n",
    "from scipy.io.wavfile import write as WAV\n",
    "import tensorflow.python.util.deprecation as deprecation\n",
    "start_time = time.time()\n",
    "tf_v1.compat.v1.disable_eager_execution()\n",
    "# tf.get_logger().setLevel('FATAL')\n",
    "# tf.autograph.set_verbosity(1)\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "# deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "# logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "# tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Decode a music score image with a trained model (CTC).')\n",
    "parser.add_argument('-image',  dest='image', type=str, required=True, help='Path to the input image.')\n",
    "parser.add_argument('-model', dest='model', type=str, required=True, help='Path to the trained model.')\n",
    "parser.add_argument('-vocabulary', dest='voc_file', type=str, required=True, help='Path to the vocabulary file.')\n",
    "parser.add_argument('-type',  dest='type', type=str, nargs='?', help='Path to the output type.')\n",
    "parser.add_argument('-seq',  dest='seq', type=str, nargs='?', help='Singles or sequential.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "tf_v1.reset_default_graph()\n",
    "sess = tf_v1.InteractiveSession()\n",
    "\n",
    "# Read the dictionary\n",
    "dict_file = open(args.voc_file,'r')\n",
    "dict_list = dict_file.read().splitlines()\n",
    "int2word = dict()\n",
    "for word in dict_list:\n",
    "    word_idx = len(int2word)\n",
    "    int2word[word_idx] = word\n",
    "dict_file.close()\n",
    "\n",
    "# Restore weights\n",
    "saver = tf_v1.train.import_meta_graph(args.model)\n",
    "saver.restore(sess,args.model[:-5])\n",
    "\n",
    "graph = tf_v1.get_default_graph()\n",
    "\n",
    "input = graph.get_tensor_by_name(\"model_input:0\")\n",
    "seq_len = graph.get_tensor_by_name(\"seq_lengths:0\")\n",
    "rnn_keep_prob = graph.get_tensor_by_name(\"keep_prob:0\")\n",
    "height_tensor = graph.get_tensor_by_name(\"input_height:0\")\n",
    "width_reduction_tensor = graph.get_tensor_by_name(\"width_reduction:0\")\n",
    "logits = graph.get_tensor_by_name(\"fully_connected/BiasAdd:0\")\n",
    "#logits = tf_v1.get_collection(\"logits\")[0]\n",
    "\n",
    "# Constants that are saved inside the model itself\n",
    "WIDTH_REDUCTION, HEIGHT = sess.run([width_reduction_tensor, height_tensor])\n",
    "\n",
    "decoded, _ = tf_v1.nn.ctc_greedy_decoder(logits, seq_len)\n",
    "\n",
    "path = Path(__file__).parent.absolute()\n",
    "\n",
    "mypath = Path().absolute()\n",
    "file_path = str(mypath) + '\\\\'\n",
    "file_forward = Path(args.image)\n",
    "absolute_path = Path(file_path + args.image)\n",
    "absolute_str = str(absolute_path)\n",
    "file_name = file_forward.name.split('.')[-2]\n",
    "file_ext = str(absolute_path).split('.')[1]\n",
    "#flen = len(file_parts)\n",
    "counter = 1\n",
    "all_predictions=[]\n",
    "bassclef= ['clef-F3','clef-F4','clef-F5']\n",
    "print(\"absolute initial \" + str(mypath))\n",
    "print(\"File name \" + file_name)\n",
    "print(\"File ext \" + file_ext)\n",
    "print(str(absolute_path))\n",
    "print(absolute_path.is_file())\n",
    "while absolute_path.exists():\n",
    "    file_name = absolute_str.split('.')[-2]\n",
    "    image = cv2.imread(str(absolute_path),0)\n",
    "    image = ctc_utils.resize(image, 128)\n",
    "    image = ctc_utils.normalize(image)\n",
    "    image = np.asarray(image).reshape(1,image.shape[0],-1,1)\n",
    "    seq_lengths = [ image.shape[2] / WIDTH_REDUCTION ]\n",
    "    prediction = sess.run(decoded,\n",
    "                          feed_dict={\n",
    "                              input: image,\n",
    "                              seq_len: seq_lengths,\n",
    "                              rnn_keep_prob: 1.0,\n",
    "                          })\n",
    "    str_predictions = ctc_utils.sparse_tensor_to_strs(prediction)\n",
    "    parsed_predictions = ''\n",
    "    for w in str_predictions[0]:\n",
    "        parsed_predictions += int2word[w] + '\\n' \n",
    "    absolute_path = Path(file_name[:-1] + str(counter) + '.' + file_ext)\n",
    "    counter+=1\n",
    "    #check for bass clef matches and discard result\n",
    "#     matching = [s for s in SEMANTIC if any(xs in s for xs in bassclef)]\n",
    "#     print (\"match? \" + matching)\n",
    "#     if matching:\n",
    "#         continue\n",
    "    all_predictions.append(parsed_predictions)\n",
    "    \n",
    "len(all_predictions)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    SEMANTIC = ''\n",
    "    playlist = []\n",
    "    track = 0\n",
    "    export = 0\n",
    "    directory=''\n",
    "    if (args.type == \"clean\"):\n",
    "        directory = 'Data\\\\clean\\\\'\n",
    "    elif(args.type == \"raw\"):\n",
    "        directory = 'Data\\\\raw\\\\'\n",
    "    else:\n",
    "        directory = 'Data\\\\perfect\\\\'\n",
    "            \n",
    "    for SEMANTIC in all_predictions:\n",
    "        # gets the audio file\n",
    "        audio = get_sinewave_audio(SEMANTIC)\n",
    "        # horizontally stacks the freqs    \n",
    "        audio =  np.hstack(audio)\n",
    "        # normalizes the freqs\n",
    "        audio *= 32767 / np.max(np.abs(audio))\n",
    "        #converts it to 16 bits\n",
    "        audio = audio.astype(np.int16)\n",
    "        playlist.append(audio)\n",
    "        \n",
    "        print(\"added one song to playlist\")\n",
    "        with open(directory + 'predictions'+ str(export) +'.txt', 'w') as file:\n",
    "            file.write(SEMANTIC)\n",
    "        export+=1        \n",
    "        \n",
    "    if(playlist):\n",
    "        if(args.seq == \"false\"):\n",
    "            for song in playlist:\n",
    "                output_file = directory + 'staff' + str(track) + '.wav'\n",
    "                WAV(output_file, 44100, song)\n",
    "                print(\"created wav file \" + str(time.time() - start_time))\n",
    "                track+=1\n",
    "        else:\n",
    "            output_file = directory + 'full_song.wav'\n",
    "            full_song = None\n",
    "            for song in playlist:\n",
    "                if (full_song) is None:\n",
    "                    full_song = song\n",
    "                else:\n",
    "                    full_song = np.concatenate((full_song, song))\n",
    "                #full_song = np.concatenate((full_song,song)) np.array([]).size\n",
    "            WAV(output_file, 44100, full_song)\n",
    "            print(\"printed full song\")\n",
    "            \n",
    "    print(\"PROCESS COMPLETED in: \" + str(time.time() - start_time) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 main.py C:\\Users\\aroue\\Downloads\\Documents\\@ML\\Mozart\\testcases\\02.PNG C:\\Users\\aroue\\Downloads\\Documents\\@ML\\Mozart\\newout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsHorizontal(img):\n",
    "    projected = []\n",
    "    rows, cols = img.shape\n",
    "    for i in range(rows):\n",
    "        proj_sum = 0\n",
    "        for j in range(cols):\n",
    "            if img[i][j] == 0:\n",
    "                proj_sum += 1\n",
    "        projected.append([1]*proj_sum + [0]*(cols-proj_sum))\n",
    "        if(proj_sum >= 0.9*cols):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def gray_img(img):\n",
    "    '''\n",
    "    img: rgb image\n",
    "    return: gray image, pixel values 0:255\n",
    "    '''\n",
    "    gray = rgb2gray(img)\n",
    "    if len(img.shape) == 3:\n",
    "        gray = gray*255\n",
    "    return gray\n",
    "\n",
    "def get_gray(img):\n",
    "    gray = rgb2gray(np.copy(img))\n",
    "    return gray\n",
    "\n",
    "def get_thresholded(img, thresh):\n",
    "    return 1*(img > thresh)\n",
    "\n",
    "def rotation(img, angle):\n",
    "    image = rotate(img, angle, resize=True, mode='edge')\n",
    "    return image\n",
    "\n",
    "def otsu(img):\n",
    "    '''\n",
    "    Otsu with gaussian\n",
    "    img: gray image\n",
    "    return: binary image, pixel values 0:1\n",
    "    '''\n",
    "    blur = gaussian(img)\n",
    "    otsu_bin = 255*(blur > threshold_otsu(blur))\n",
    "    return (otsu_bin/255).astype(np.int32)\n",
    "\n",
    "def show_images(images, titles=None):\n",
    "    n_ims = len(images)\n",
    "    if titles is None:\n",
    "        titles = ['(%d)' % i for i in range(1, n_ims + 1)]\n",
    "    fig = plt.figure()\n",
    "    n = 1\n",
    "    for image, title in zip(images, titles):\n",
    "        a = fig.add_subplot(1, n_ims, n)\n",
    "        if image.ndim == 2:\n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        plt.savefig(r'C:\\Users\\aroue\\Downloads\\Documents\\@ML\\Mozart\\newout\\foos.png')\n",
    "        a.set_title(title)\n",
    "        plt.axis('off')\n",
    "        n += 1\n",
    "    #fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n",
    "    plt.show()\n",
    "    \n",
    "def draw_staff(img,row_positions):\n",
    "    image = np.copy(img)\n",
    "    for x in range (len(row_positions)):\n",
    "        print(int(row_positions[x]))\n",
    "        image[int(row_positions[x]),:] = 0\n",
    "    return image\n",
    "\n",
    "def deskew(image):\n",
    "    edges = canny(image, low_threshold=50, high_threshold=150, sigma=2)\n",
    "    harris = corner_harris(edges)\n",
    "    tested_angles = np.linspace(-np.pi / 2, np.pi / 2, 360)\n",
    "    h, theta, d = hough_line(harris, theta=tested_angles)\n",
    "    out, angles, d = hough_line_peaks(h, theta, d)\n",
    "    rotation_number = np.average(np.degrees(angles))\n",
    "    if rotation_number < 45 and rotation_number != 0:\n",
    "        rotation_number += 90\n",
    "    return rotation_number\n",
    "\n",
    "def get_closer(img):\n",
    "    rows = []\n",
    "    cols = []\n",
    "    for x in range(16):\n",
    "        no = 0\n",
    "        for col in range(x*img.shape[0]//16, (x+1)*img.shape[0]//16):\n",
    "            for row in range(img.shape[1]):\n",
    "                if img[col][row] == 0:\n",
    "                    no += 1\n",
    "        if no >= 0.01*img.shape[1]*img.shape[0]//16:\n",
    "            rows.append(x*img.shape[0]//16)\n",
    "    for x in range(16):\n",
    "        no = 0\n",
    "        for row in range(x*img.shape[1]//16, (x+1)*img.shape[1]//16):\n",
    "            for col in range(img.shape[0]):\n",
    "                if img[col][row] == 0:\n",
    "                    no += 1\n",
    "        if no >= 0.01*img.shape[0]*img.shape[1]//16:\n",
    "            cols.append(x*img.shape[1]//16)\n",
    "    new_img = img[rows[0]:min(img.shape[0], rows[-1]+img.shape[0]//16),\n",
    "                  cols[0]:min(img.shape[1], cols[-1]+img.shape[1]//16)]\n",
    "    return new_img\n",
    "\n",
    "import skimage.io as io\n",
    "from skimage.feature import corner_harris\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.feature import canny\n",
    "from skimage.transform import probabilistic_hough_line, hough_line, rotate, hough_line_peaks\n",
    "from staff import calculate_thickness_spacing, remove_staff_lines, coordinator\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import threshold_otsu, gaussian, median\n",
    "from glob import glob\n",
    "from segmenter import Segmenter\n",
    "\n",
    "#input_path=\"C:/Users/aroue/Downloads/Documents/@ML/Mozart/testcases/02.PNG\"\n",
    "input_path=r\"C:\\Users\\aroue\\Downloads\\Documents\\@ML\\Sheet Music\\zanarkand.jpg\"\n",
    "#img_path = sorted(glob(f'{input_path}/*'))\n",
    "\n",
    "img_name = input_path.split('/')[-1]\n",
    "imgs_path = input_path[:-len(img_name)]\n",
    "print(imgs_path)\n",
    "print(input_path)\n",
    "\n",
    "img = io.imread(f'{imgs_path}{img_name}')\n",
    "img = gray_img(img)\n",
    "\n",
    "original = img.copy()\n",
    "gray = get_gray(img)\n",
    "\n",
    "\n",
    "bin_img = get_thresholded(gray, threshold_otsu(gray))\n",
    "#show_images([bin_img])\n",
    "\n",
    "segmenter = Segmenter(bin_img)\n",
    "#segmenter = Segmenter(gray)\n",
    "#print(segmenter)\n",
    "#show_images([imgs_with_staff[0]])\n",
    "\n",
    "#show_images([imgr])\n",
    "imgs_spacing = []\n",
    "imgs_rows = []\n",
    "coord_imgs = []\n",
    "horizontal = IsHorizontal(img)\n",
    "if horizontal == False:\n",
    "    theta = deskew(img)\n",
    "    img = rotation(img,theta)\n",
    "    img = get_gray(img)\n",
    "    img = get_thresholded(img, threshold_otsu(img))\n",
    "    img = get_closer(img)\n",
    "    horizontal = IsHorizontal(img)\n",
    "\n",
    "imgr = segmenter.no_staff_img\n",
    "imgs_with_staff = segmenter.regions_with_staff\n",
    "    \n",
    "for i, img in enumerate(imgs_with_staff):\n",
    "    spacing, rows, no_staff_img = coordinator(img,horizontal)\n",
    "    imgs_rows.append(rows)\n",
    "    imgs_spacing.append(spacing)\n",
    "    coord_imgs.append(no_staff_img)\n",
    "for i, img in enumerate(coord_imgs):\n",
    "    new_img = draw_staff(img,imgs_rows[i])\n",
    "    show_images([img,new_img], ['Binary','new'])  \n",
    "    cv2.imwrite(f'{img_name}_without_staff_{i}.png', np.array(255*img).astype(np.uint8))\n",
    "    cv2.imwrite(f'{img_name}_with_new_staff_{i}.png', np.array(255*new_img).astype(np.uint8))\n",
    "#imgs_without_staff = segmenter.regions_without_staff\n",
    "\n",
    "# for i, img in enumerate(imgs_with_staff):\n",
    "#     show_images([imgs_with_staff[i]])\n",
    "#     break\n",
    "    \n",
    "# for i, img in enumerate(imgs_with_staff):\n",
    "#     show_images([img, imgs_with_staff[i]])\n",
    "#     break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BACKUP\n",
    "from rle import *\n",
    "from commonfunctions import *\n",
    "from staff import calculate_thickness_spacing, remove_staff_lines\n",
    "\n",
    "\n",
    "class Segmenter(object):\n",
    "    def __init__(self, bin_img):\n",
    "        self.bin_img = bin_img\n",
    "        self.rle, self.vals = hv_rle(self.bin_img)\n",
    "        self.most_common = get_most_common(self.rle)\n",
    "        self.thickness, self.spacing = calculate_thickness_spacing(\n",
    "            self.rle, self.most_common)\n",
    "        self.thick_space = self.thickness + self.spacing\n",
    "        self.no_staff_img = remove_staff_lines(\n",
    "            self.rle, self.vals, self.thickness, self.bin_img.shape)\n",
    "\n",
    "        self.segment()\n",
    "\n",
    "    def open_region(self, region):\n",
    "        thickness = np.copy(self.thickness)\n",
    "        # if thickness % 2 == 0:\n",
    "        #     thickness += 1\n",
    "        return opening(region, np.ones((thickness, thickness)))\n",
    "\n",
    "    def segment(self):\n",
    "        self.line_indices = get_line_indices(histogram(self.bin_img, 0.8))\n",
    "        if len(self.line_indices) < 10:\n",
    "            self.regions_without_staff = [\n",
    "                np.copy(self.open_region(self.no_staff_img))]\n",
    "            self.regions_with_staff = [np.copy(self.bin_img)]\n",
    "            return\n",
    "\n",
    "        generated_lines_img = np.copy(self.no_staff_img)\n",
    "        lines = []\n",
    "        for index in self.line_indices:\n",
    "            line = ((0, index), (self.bin_img.shape[1]-1, index))\n",
    "            lines.append(line)\n",
    "\n",
    "        end_of_staff = []\n",
    "        for index, line in enumerate(lines):\n",
    "            if index > 0 and (line[0][1] - end_of_staff[-1][1] < 4*self.spacing):\n",
    "                pass\n",
    "            else:\n",
    "                p1, p2 = line\n",
    "                x0, y0 = p1\n",
    "                x1, y1 = p2\n",
    "                end_of_staff.append((x0, y0, x1, y1))\n",
    "\n",
    "        box_centers = []\n",
    "        spacing_between_staff_blocks = []\n",
    "        for i in range(len(end_of_staff)-1):\n",
    "            spacing_between_staff_blocks.append(\n",
    "                end_of_staff[i+1][1] - end_of_staff[i][1])\n",
    "            if i % 2 == 0:\n",
    "                offset = (end_of_staff[i+1][1] - end_of_staff[i][1])//2\n",
    "                center = end_of_staff[i][1] + offset\n",
    "                box_centers.append((center, offset))\n",
    "\n",
    "        max_staff_dist = np.max(spacing_between_staff_blocks)\n",
    "        max_margin = max_staff_dist // 2\n",
    "        margin = max_staff_dist // 10\n",
    "\n",
    "        end_points = []\n",
    "        regions_without_staff = []\n",
    "        regions_with_staff = []\n",
    "        for index, (center, offset) in enumerate(box_centers):\n",
    "            y0 = int(center) - max_margin - offset + margin\n",
    "            y1 = int(center) + max_margin + offset - margin\n",
    "            end_points.append((y0, y1))\n",
    "\n",
    "            region = self.bin_img[y0:y1, 0:self.bin_img.shape[1]]\n",
    "            regions_with_staff.append(region)\n",
    "            staff_block = self.no_staff_img[y0:y1,\n",
    "                                            0:self.no_staff_img.shape[1]]\n",
    "\n",
    "            regions_without_staff.append(self.open_region(staff_block))\n",
    "\n",
    "        self.regions_without_staff = regions_without_staff\n",
    "        self.regions_with_staff = regions_with_staff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "STARTING SEGMENTATION\n",
      "Image generated\n",
      "Image generated\n",
      "Image generated\n",
      "Image generated\n",
      "Image generated\n",
      "Image generated\n",
      "COMPLETE\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_image(data, outputname, size=(128, 200), dpi=80):\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(size)\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    plt.gray()\n",
    "    ax.imshow(data, aspect='equal')\n",
    "    plt.show()\n",
    "    #plt.savefig(outputname, dpi=dpi)\n",
    "\n",
    "def binarize_image(img):\n",
    "    mat = cv2.imdecode(img, cv2.IMREAD_UNCHANGED)\n",
    "    #blur = cv2.bilateralFilter(mat, 9, 128, 128) \n",
    "    #blur = cv2.medianBlur(mat,5)\n",
    "    blur = cv2.GaussianBlur(mat,(3,3),0)\n",
    "    #blur = cv2.medianBlur(mat, 5) \n",
    "    #ret,bin_img = cv2.threshold(img,127,255,cv2.THRESH_BINARY)\n",
    "    #bin_img = cv2.adaptiveThreshold(mat,255,cv2.ADAPTIVE_THRESH_MEAN_C,\\\n",
    "    #        cv2.THRESH_BINARY,11,2)\n",
    "    #bin_img = cv2.adaptiveThreshold(mat,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "    #        cv2.THRESH_BINARY,11,2)\n",
    "    ret3, bin_img = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    #ret1, imf = cv2.threshold(mat, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    #ret1, th1 = cv2.threshold(mat, 127, 255, cv2.THRESH_BINARY)\n",
    "    return cv2.imencode(\".jpg\", bin_img)\n",
    "\n",
    "\n",
    "import cv2\n",
    "from commonfunctions import *\n",
    "from pre_processing import *\n",
    "from staff import calculate_thickness_spacing, remove_staff_lines, coordinator\n",
    "from segmenter import Segmenter\n",
    "from connected_componentes import  *\n",
    "from fit import predict\n",
    "from box import Box\n",
    "import skimage.io as io\n",
    "import glob\n",
    "import cv2\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from wand.image import Image\n",
    "from wand.display import display\n",
    "from imutils import resize as im_resize\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from skimage.morphology import skeletonize, thin\n",
    "from skimage.filters import threshold_otsu, gaussian, median, threshold_yen\n",
    "\n",
    "input_path=r\"C:\\Users\\aroue\\Downloads\\Documents\\@ML\\Sheet Music\\goodsheet\\pgws2.png\"\n",
    "img_name = input_path.split('/')[-1]\n",
    "imgs_path = input_path[:-len(img_name)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "img = io.imread(f'{imgs_path}{img_name}')\n",
    "img = cv2.imread(input_path,0)\n",
    "img_buffer=None\n",
    "imgf=None\n",
    "imgmat=None\n",
    "with Image.from_array(img) as im:\n",
    "    img_buffer = np.asarray(bytearray(im.make_blob(\"JPEG\")), dtype=np.uint8)\n",
    "    ret, mat = binarize_image(img_buffer)\n",
    "    with Image(blob=mat) as timg:\n",
    "        imgf = mat\n",
    "        #imgmat = mat\n",
    "        timg.save(filename=\"otsu.jpg\")\n",
    "        timg.deskew(0.4*im.quantum_range)\n",
    "        timg.save(filename=\"otsu2.jpg\")\n",
    "        imgf = np.array(timg)\n",
    "        img_buffer = np.asarray(bytearray(timg.make_blob(\"JPEG\")), dtype=np.uint8)\n",
    "        imgmat = cv2.imdecode(img_buffer, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "\n",
    "# with Image.from_array(imgf) as im:\n",
    "#     im.deskew(0.4*im.quantum_range)\n",
    "#     im.save(filename=\"otsu2.jpg\")\n",
    "#     imgf = np.array(im)\n",
    "print(type(imgmat))\n",
    "print(type(imgf))\n",
    "# with Image.from_array(img) as img:\n",
    "#     #img.background_color = Color('white')\n",
    "#     #img.format        = 'tif'\n",
    "#     img.alpha_channel = 'remove'\n",
    "#     img.deskew(0.4*img.quantum_range)\n",
    "#     # Fill image buffer with numpy array from blob\n",
    "#     img_buffer=np.asarray(bytearray(img.make_blob()), dtype=np.uint8)\n",
    "\n",
    "# if img_buffer is not None:\n",
    "#     retval = cv2.imdecode(img_buffer, cv2.IMREAD_UNCHANGED)\n",
    "#     ret, imgf = cv2.threshold(retval, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "#\n",
    "\n",
    "#skew = Image.from_array(imgf)\n",
    "#png.deskew(0.4*png.quantum_range)\n",
    "\n",
    "# plt.subplot(3,1,1), plt.imshow(imgf,cmap = 'gray')\n",
    "# plt.title('Original Noisy Image'), plt.xticks([]), plt.yticks([])\n",
    "# skew = Image.from_array(imgf):\n",
    "#     png.deskew(0.4*png.quantum_range)\n",
    "    #img.save(filename='table_deskew.png')\n",
    "    #display(png)\n",
    "    \n",
    "# with Image(imgf) as png:\n",
    "#     png.deskew(0.4*png.quantum_range)\n",
    "#     #img.save(filename='table_deskew.png')\n",
    "#     display(png)\n",
    "\n",
    "\n",
    "# horizontal = IsHorizontal(imgf)\n",
    "# print(\"horizontal?\" + str(horizontal))\n",
    "# if horizontal == False:\n",
    "#     theta = deskew(imgf)\n",
    "#     imgf = rotation(imgf,theta)\n",
    "#     imgf = get_gray(imgf)\n",
    "#     imgf = get_thresholded(imgf, threshold_otsu(imgf))\n",
    "#     imgf = get_closer(imgf)\n",
    "#     horizontal = IsHorizontal(imgf)\n",
    "\n",
    "    #show_images([img])\n",
    "# plt.title('Otsu Horizontal'), plt.xticks([]), plt.yticks([])\n",
    "# plt.show()\n",
    "\n",
    "# plt.subplot(3,1,1), plt.imshow(img,cmap = 'gray')\n",
    "# plt.title('Original Noisy Image'), plt.xticks([]), plt.yticks([])\n",
    "# #plt.subplot(3,1,2), plt.hist(img.ravel(), 256)\n",
    "# #plt.axvline(x=ret, color='r', linestyle='dashed', linewidth=2)\n",
    "# #plt.title('Histogram'), plt.xticks([]), plt.yticks([])\n",
    "# plt.subplot(3,1,3), plt.imshow(imgf,cmap = 'gray')\n",
    "# plt.title('Otsu thresholding'), plt.xticks([]), plt.yticks([])\n",
    "# plt.show()\n",
    "\n",
    "#img = gray_img(img)\n",
    "\n",
    "# original = img.copy()\n",
    "# img = otsuMethod(img)\n",
    "# print(type(img))\n",
    "# im = Image.fromarray(img)\n",
    "# rgb_im = im.convert('RGB')\n",
    "\n",
    "#rgb_im.save(\"your_file.png\")\n",
    "#gray = get_gray(img)\n",
    "#imgf = get_gray(imgf)\n",
    "#imgf = get_thresholded(imgmat, threshold_otsu(imgmat))\n",
    "# g = threshold_yen(gray)\n",
    "# print(g)\n",
    "# rag = 1*(gray > 230)\n",
    "\n",
    "#show_images([gray, original], ['Gray', 'Binary'])\n",
    "print(\"STARTING SEGMENTATION\")\n",
    "#show_images([bin_img[0]])\n",
    "#img = Image.fromarray(bin_img, 'RGB')\n",
    "#img.save('my.png')\n",
    "#img.show()\n",
    "\n",
    "#gray = 1*gray[gray > 250] = 1\n",
    "# plt.rcParams[\"figure.figsize\"] = (20,29)\n",
    "# plt.gray()\n",
    "# plt.imshow(rag, interpolation='nearest')\n",
    "# plt.show()\n",
    "# print(gray[0])\n",
    "#show_images([imgf])\n",
    "#imgf = get_thresholded(imgmat, threshold_otsu(imgmat))\n",
    "imgmat = get_thresholded(imgmat, 245)\n",
    "segmenter = Segmenter(imgmat)\n",
    "imgs_with_staff = segmenter.regions_with_staff\n",
    "show_images([imgs_with_staff[0]])\n",
    "#imgs_without_staff = segmenter.regions_without_staff\n",
    "\n",
    "#show_images([imgs_without_staff[0]])\n",
    "\n",
    "for i, img in enumerate(imgs_with_staff):\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,15)\n",
    "    plt.gca().set_axis_off()\n",
    "    plt.gca().set_title(\"\")\n",
    "    fig=plt.imshow(imgs_with_staff[i],interpolation='nearest')\n",
    "    #fig.axes.get_xaxis().set_visible(False)\n",
    "    #plt.axis(\"off\")\n",
    "    #plt.show()\n",
    "    #show_images([imgs_with_staff[i]])\n",
    "    plt.savefig('output\\slice'+str(i)+'.png',\n",
    "    bbox_inches='tight', pad_inches=0, format='png', dpi=600)\n",
    "    print(\"Image generated\")\n",
    "\n",
    "print(\"COMPLETE\")\n",
    "#     #make_image(imgs_with_staff[i], '/tmp/out.png')\n",
    "    \n",
    "#     plt.gray()\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, \n",
    "#                 hspace = 0, wspace = 0)\n",
    "#     plt.margins(0,0)\n",
    "    #plt.imshow(imgs_with_staff[i], interpolation='nearest')\n",
    "\n",
    "#blur = cv2.bilateralFilter(mat, 9, 128, 128) \n",
    "    #blur = cv2.medianBlur(mat,5)\n",
    "    #blur = cv2.GaussianBlur(mat,(3,3),0)\n",
    "    #blur = cv2.medianBlur(mat, 5) \n",
    "    #ret,bin_img = cv2.threshold(img,127,255,cv2.THRESH_BINARY)\n",
    "    #bin_img = cv2.adaptiveThreshold(mat,255,cv2.ADAPTIVE_THRESH_MEAN_C,\\\n",
    "    #        cv2.THRESH_BINARY,11,2)\n",
    "    #bin_img = cv2.adaptiveThreshold(mat,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "    #        cv2.THRESH_BINARY,11,2)\n",
    "    \n",
    "    #ret1, imf = cv2.threshold(mat, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    #ret1, th1 = cv2.threshold(mat, 127, 255, cv2.THRESH_BINARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make tf 2.0 compatible with tf1.0 code, we disable the tf2.0 functionalities\n",
    "#tf.disable_eager_execution()\n",
    "def appending(list, element):\n",
    "    return np.concatenate((list, [element]), axis=0) if list is not None else [element]\n",
    "\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "import ctc_utils\n",
    "import numpy as np\n",
    "from segmenter.slicer import Slice\n",
    "import silence_tensorflow.auto\n",
    "import tensorflow as tf\n",
    "import simpleaudio as sa\n",
    "from pathlib import Path\n",
    "from midi.player import *\n",
    "import tensorflow.compat.v1 as tf_v1\n",
    "from scipy.io.wavfile import write as WAV\n",
    "import tensorflow.python.util.deprecation as deprecation\n",
    "start_time = time.time()\n",
    "tf_v1.compat.v1.disable_eager_execution()\n",
    "# tf.get_logger().setLevel('FATAL')\n",
    "# tf.autograph.set_verbosity(1)\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "# deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "# logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "# tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Decode a music score image with a trained model (CTC).')\n",
    "parser.add_argument('-image',  dest='image', type=str, required=True, help='Path to the input image.')\n",
    "parser.add_argument('-model', dest='model', type=str, required=True, help='Path to the trained model.')\n",
    "parser.add_argument('-vocabulary', dest='voc_file', type=str, required=True, help='Path to the vocabulary file.')\n",
    "parser.add_argument('-type',  dest='type', type=str, nargs='?', help='Path to the output type.')\n",
    "parser.add_argument('-seq',  dest='seq', type=str, nargs='?', help='Singles or sequential.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "tf_v1.reset_default_graph()\n",
    "sess = tf_v1.InteractiveSession()\n",
    "\n",
    "# Read the dictionary\n",
    "dict_file = open(args.voc_file,'r')\n",
    "dict_list = dict_file.read().splitlines()\n",
    "int2word = dict()\n",
    "for word in dict_list:\n",
    "    word_idx = len(int2word)\n",
    "    int2word[word_idx] = word\n",
    "dict_file.close()\n",
    "\n",
    "# Restore weights\n",
    "saver = tf_v1.train.import_meta_graph(args.model)\n",
    "saver.restore(sess,args.model[:-5])\n",
    "\n",
    "graph = tf_v1.get_default_graph()\n",
    "\n",
    "input = graph.get_tensor_by_name(\"model_input:0\")\n",
    "seq_len = graph.get_tensor_by_name(\"seq_lengths:0\")\n",
    "rnn_keep_prob = graph.get_tensor_by_name(\"keep_prob:0\")\n",
    "height_tensor = graph.get_tensor_by_name(\"input_height:0\")\n",
    "width_reduction_tensor = graph.get_tensor_by_name(\"width_reduction:0\")\n",
    "logits = graph.get_tensor_by_name(\"fully_connected/BiasAdd:0\")\n",
    "\n",
    "# Constants that are saved inside the model itself\n",
    "WIDTH_REDUCTION, HEIGHT = sess.run([width_reduction_tensor, height_tensor])\n",
    "\n",
    "decoded, _ = tf_v1.nn.ctc_greedy_decoder(logits, seq_len)\n",
    "\n",
    "path = Path(__file__).parent.absolute()\n",
    "\n",
    "mypath = Path().absolute()\n",
    "file_path = str(mypath) + '\\\\'\n",
    "file_forward = Path(args.image)\n",
    "absolute_path = Path(file_path + args.image)\n",
    "absolute_str = str(absolute_path)\n",
    "file_name = file_forward.name.split('.')[-2]\n",
    "file_ext = str(absolute_path).split('.')[1]\n",
    "counter = 1\n",
    "all_predictions=[]\n",
    "bassclef= ['clef-F3','clef-F4','clef-F5']\n",
    "print(\"absolute initial \" + str(mypath))\n",
    "print(\"File name \" + file_name)\n",
    "print(\"File ext \" + file_ext)\n",
    "print(str(absolute_path))\n",
    "print(absolute_path.is_file())\n",
    "while absolute_path.exists():\n",
    "    file_name = absolute_str.split('.')[-2]\n",
    "    image = cv2.imread(str(absolute_path),0)\n",
    "    image = ctc_utils.resize(image, 128)\n",
    "    image = ctc_utils.normalize(image)\n",
    "    image = np.asarray(image).reshape(1,image.shape[0],-1,1)\n",
    "    seq_lengths = [ image.shape[2] / WIDTH_REDUCTION ]\n",
    "    prediction = sess.run(decoded,\n",
    "                          feed_dict={\n",
    "                              input: image,\n",
    "                              seq_len: seq_lengths,\n",
    "                              rnn_keep_prob: 1.0,\n",
    "                          })\n",
    "    str_predictions = ctc_utils.sparse_tensor_to_strs(prediction)\n",
    "    parsed_predictions = ''\n",
    "    for w in str_predictions[0]:\n",
    "        parsed_predictions += int2word[w] + '\\n' \n",
    "    absolute_path = Path(file_name[:-1] + str(counter) + '.' + file_ext)\n",
    "    counter+=1\n",
    "    #check for bass clef matches and discard result\n",
    "#     matching = [s for s in SEMANTIC if any(xs in s for xs in bassclef)]\n",
    "#     print (\"match? \" + matching)\n",
    "#     if matching:\n",
    "#         continue\n",
    "    all_predictions.append(parsed_predictions)\n",
    "    \n",
    "len(all_predictions)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    SEMANTIC = ''\n",
    "    playlist = []\n",
    "    track = 0\n",
    "    export = 0\n",
    "    directory=''\n",
    "    if (args.type == \"clean\"):\n",
    "        directory = 'Data\\\\clean\\\\'\n",
    "    elif(args.type == \"raw\"):\n",
    "        directory = 'Data\\\\raw\\\\'\n",
    "    else:\n",
    "        directory = 'Data\\\\perfect\\\\'\n",
    "            \n",
    "    for SEMANTIC in all_predictions:\n",
    "        # gets the audio file\n",
    "        audio = get_sinewave_audio(SEMANTIC)\n",
    "        # horizontally stacks the freqs    \n",
    "        audio =  np.hstack(audio)\n",
    "        # normalizes the freqs\n",
    "        audio *= 32767 / np.max(np.abs(audio))\n",
    "        #converts it to 16 bits\n",
    "        audio = audio.astype(np.int16)\n",
    "        playlist.append(audio)\n",
    "        \n",
    "        print(\"added one song to playlist\")\n",
    "        with open(directory + 'predictions'+ str(export) +'.txt', 'w') as file:\n",
    "            file.write(SEMANTIC)\n",
    "        export+=1        \n",
    "        \n",
    "    if(playlist):\n",
    "        if(args.seq == \"false\"):\n",
    "            for song in playlist:\n",
    "                output_file = directory + 'staff' + str(track) + '.wav'\n",
    "                WAV(output_file, 44100, song)\n",
    "                print(\"created wav file \" + str(time.time() - start_time))\n",
    "                track+=1\n",
    "        else:\n",
    "            output_file = directory + 'full_song.wav'\n",
    "            full_song = None\n",
    "            for song in playlist:\n",
    "                if (full_song) is None:\n",
    "                    full_song = song\n",
    "                else:\n",
    "                    full_song = np.concatenate((full_song, song))\n",
    "                #full_song = np.concatenate((full_song,song)) np.array([]).size\n",
    "            WAV(output_file, 44100, full_song)\n",
    "            print(\"printed full song\")\n",
    "            \n",
    "    print(\"PROCESS COMPLETED in: \" + str(time.time() - start_time) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
